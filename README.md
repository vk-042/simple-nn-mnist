# ğŸ§  Simple Neural Network from Scratch (MNIST Digit Classification)

This project implements a simple neural network **from scratch using NumPy**, trained on the MNIST dataset to classify handwritten digits (0â€“9). No high-level libraries like TensorFlow or PyTorch were used.

## ğŸš€ Overview
- Pure NumPy-based implementation
- Implements core neural network concepts:
  - Forward propagation
  - Softmax activation
  - One-hot encoding
  - Batch training
  - Model evaluation

## ğŸ“Š Results
Achieved over **70% accuracy** on the MNIST test set using:
- 1 hidden layer
- ReLU activation
- Manual vectorized training loop

## ğŸ› ï¸ Tech Stack
- Python
- NumPy
- Matplotlib (optional for visualization)
- Jupyter Notebook

## ğŸ“ Files
- `Simple Neural Network.ipynb` â€“ Core notebook
- `README.md` â€“ Project description

## ğŸ“¦ Installation & Usage
```bash
git clone https://github.com/vk-042/simple-nn-mnist.git
cd simple-nn-mnist
pip install numpy
jupyter notebook "Simple Neural Network.ipynb"

## ğŸ“Œ Output Snapshot

The test accuracy across 10 epochs:

epoch= 0 â†’ accuracy: 79.74%
epoch= 1 â†’ accuracy: 79.81%
epoch= 2 â†’ accuracy: 79.82%
epoch= 3 â†’ accuracy: 79.85%
epoch= 4 â†’ accuracy: 79.88%
epoch= 5 â†’ accuracy: 79.85%
epoch= 6 â†’ accuracy: 79.81%
epoch= 7 â†’ accuracy: 79.84%
epoch= 8 â†’ accuracy: 79.89%
epoch= 9 â†’ accuracy: 79.88%


This model was trained using only **NumPy** with manual forward propagation and weight updates.




